---
title: "Statistical Rethinking: Chapter 4"
output: html_notebook
---

# Linear Models

## Why normal distributions are normal

### Normal by Addition

Lets plot the distribution for 1,000 coin flips where each person gets a random number from a list of 16 between -1 and 1.

```{r}
pos <- replicate(n = 1000, expr = sum(runif(16, -1, 1)))

hist(pos)
plot(density(pos))
```

Any process that adds random values from the same distribution converges to a normal. Whatever the average value of the source distribution, each sample ffrom it can be thought of as a fluctuation from that average value. When we add these together, they begin to cancel one another out. 

### Normal by multiplication

Suppose the growth rate of an organism is influenced by a dozen loci, each with several alleles that code for more growth. SUppose they all interact with one another, suhc that each increase growth by a percentage. This means that their effects multiply, rathen than add.

```{r}
library(rethinking)

prod(1 + runif(12, 0, 0.1))

growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
dens(growth, norm.comp = TRUE)
```

Multiplicative terms approximate normality by addition if the values are small enough:

```{r}
big <- replicate(10000, prod(1 + runif(12, 0, 0.5)))
dens(big, norm.comp = TRUE)

small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))
dens(small, norm.comp = TRUE)
```

### Normal by log-multiplacation

Big deviations produce a normal on the log scale

```{r}
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
dens(log.big, norm.comp = TRUE)
```

## A language for describing models 

1. First, we recgonize a set of measurements that we hope to predict or understand, the outcome variable or variables.

2. for each of these outcome variables, we define a likelihood distribution that defines the plausibility of individual observations. In linear regression, this distribution is always Gaussian.

3. Then we recognize a set of other measurements that we hope to use to predict or understand the outcome. Call these **predictor** variables.

4. We relate the exact shape of the likelihood distribution - its precise location and varance and other aspects of its shape, if it has them - to the predictor variables. In choosing a way to relate the predictors to the outcomes, we are forced to name and define all of the parameters of the model.

5. Finally, we choose priors for all of the parameters in the model. These priors define the initial information state of the model, before seeing the data. 

$$
\begin{aligned}
\text{outcome}_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta \text{ x predictor}_i \\
\beta &\sim \text{Normal}(0, 10) \\
\sigma &\sim \text{HalfCauchy}(0, 1)
\end{aligned}
$$

## A Gaussian model of height

### The data

```{r}
library(rethinking)
data("Howell1")
d <- Howell1

str(d)
```

Filter the data down to individuals of age 18 or greater with:

```{r}
d2 <- d[d$age >= 18, ]

dens(d2$height)
```

Define the heights as normally distributed with a mean and varance along with priors:

$$
\begin{aligned}
h_i &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(178, 20) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{aligned}
$$

Let's plot the priors:

```{r}
curve(dnorm(x, 178, 20), from = 100, to = 250)

curve(dnorm(x, 0, 50), from = -10, to = 60)
```

```{r}
sample_mu <- rnorm(10000, 178, 20)
sample_sigma <- runif(10000, 0, 50)
prior_h <- rnorm(10000, sample_mu, sample_sigma)
dens(prior_h)
```

### Grid approximationo the posterior distribution

```{r}
mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from = 4, to = 9, length.out = 200)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(1:nrow(post), function(i) sum(dnorm(
  d2$height,
  mean = post$mu[i],
  sd = post$sigma[i],
  log = TRUE
)))
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))

contour_xyz(post$mu, post$sigma, post$prob)
image_xyz(post$mu, post$sigma, post$prob)
```

### Sampling from the posterior

First, we randomly sample row numbers in `post` in proportion to the values in `post$prob`. Then we pull out the parameter values on those randomly sampled rows. 

```{r}
sample.rows <- sample(1:nrow(post), 
                      size = 10000, 
                      replace = TRUE, 
                      prob = post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]

plot(sample.mu, sample.sigma, cex = 0.5, pch = 16, col = col.alpha(rangi2, 0.1))

dens(sample.mu)
dens(sample.sigma)

HPDI(sample.mu)
HPDI(sample.sigma)
```

### Fitting the model with `map`

```{r}
data("Howell1")
d <- Howell1
d2 <- d[d$age >= 18, ]

# Define the Model
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

# Fit the model
m4.1 <- map(flist, data = d2)
m4.1.stan.rt <- map2stan(m4.1)
stancode(m4.1.stan.rt)

# Results
precis(m4.1)
precis(m4.1.stan.rt)
```

### Fitting the model with Stan

Same model in raw Stan:

```{stan output.var = "m4.1.stan"}
data {
  int<lower = 1> N; // number of data items
  vector[N] height; // outcome vector
}

parameters {
  real mu; // intercept
  real sigma; // error scale
}

model {
  sigma ~ uniform(0, 50); //prior
  mu ~ normal(178, 20); //prior
  height ~ normal(mu, sigma); // likelihood
}
```

Now estimate the model:

```{r}
dat <- list(N = NROW(d2),
            height = d2$height)

m4.1.stan.fit <- sampling(m4.1.stan,
                          data = dat)
precis(m4.1.stan.fit)
```


Now lets change the prior on $\mu$:

```{r}
m4.2 <- map(
  alist(height ~ dnorm(mu, sigma),
        mu ~ dnorm(178, 0.1),
        sigma ~ dunif(0, 50)),
  data = d2
)

# Results
precis(m4.2)
```

```{stan output.var = "m4.2.stan"}
data {
  int<lower = 1> N; // number of data items
  vector[N] height; // outcome vector
}

parameters {
  real mu;
  real sigma;
}

model {
  sigma ~ uniform(0, 50);
  mu ~ normal(178, 0.1);
  height ~ normal(mu, sigma);
}
```

```{r}
m4.2.stan.fit <- sampling(m4.2.stan,
                          data = dat)
precis(m4.2.stan.fit)
```


Now, let's sample vectors of values from a multi-dimensional Gaussian distribution:

```{r}
post <- extract.samples(m4.1, n = 10000)
head(post)

precis(post)

plot(post)
```

## Adding a predictor

```{r}
plot(d$height ~ d$weight)
```

```{r}
m4.3 <- map(
  alist(height ~ dnorm(mu, sigma),
        mu <- a + b * weight,
        a ~ dnorm(156, 100),
        b ~ dnorm(0, 10),
        sigma ~ dunif(0, 50)),
  data = d2
)
m4.3.stan.rt <- map2stan(m4.3)
stancode(m4.3.stan.rt)
# Results
precis(m4.3, corr = TRUE)
precis(m4.3.stan.rt)
```

```{stan output.var = "m4.3.stan"}
data {
  int<lower = 1> N;
  vector[N] height;
  vector[N] weight;
}

parameters {
  real a;
  real b;
  real sigma;
}

model {
  vector[N] mu;
  
  // Linear Model
  mu = a + b * weight;
  
  // Priors
  sigma ~ uniform(0, 50);
  a ~ normal(156, 100);
  b ~ normal(0, 10);
  
  // Likelihood
  height ~ normal(mu, sigma);
}
```

```{r}
dat <- list(N = NROW(d2),
            height = d2$height,
            weight = d2$weight)

m4.3.stan.fit <- sampling(m4.3.stan,
                          data = dat,
                          warmup = 300)
precis(m4.3.stan.fit)
```



Strong correlations between variables can be problematic. One way to fit this is to center the variables:

```{r}
d2$weight.c <- d2$weight - mean(d2$weight)
mean(d2$weight.c)

# Now refit the model
m4.4 <- map(
  alist(height ~ dnorm(mu, sigma),
        mu <- a + b * weight.c,
        a ~ dnorm(156, 100),
        b ~ dnorm(0, 10),
        sigma ~ dunif(0, 50)),
  data = d2
)

# Results
precis(m4.4, corr = TRUE)
```

```{stan output.var = "m4.4.stan"}
data {
  int<lower = 1> N;
  vector[N] height;
  vector[N] weight_c;
}

parameters {
  real a;
  real b;
  real sigma;
}

model {
  // linear model
  vector[N] mu;
  mu = a + b * weight_c;
  
  // priors
  a ~ normal(156, 100);
  b ~ normal(0, 10);
  sigma ~ uniform(0, 50);
  
  // likelihood
  height ~ normal(mu, sigma);
}
```

```{r}
d2$weight.c <- d2$weight - mean(d2$weight)
dat <- list(N = NROW(d2),
            height = d2$height,
            weight_c = d2$weight.c)

m4.4.stan.fit <- sampling(m4.4.stan,
                          data = dat)
m4.4.stan.fit

# Plotting posterior inference against the data
post <- as.data.frame(m4.3.stan.fit)

ggplot() +
  geom_point(data = d2, aes(x = weight, y = height), shape = 1, color = "blue") +
  geom_abline(intercept = mean(post$a), slope = mean(post$b)) +
  lims(y = c(130, 180))
```


To superimpose the MAP values for mean height over the actual data:

```{r}
plot(height ~ weight, data = d2)
abline(a = coef(m4.3)["a"], b = coef(m4.3)["b"])

# Incorporate uncertanty
post <- extract.samples(m4.3)
head(post)
```

Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by `vcov(m4.3)`. Each row defines a line. The average of very many of these lines is the MAP line. Let's begin with just the first 10 cases:

```{r}
N <- 300
dN <- d2[1:N, ]
mN <- map(
  alist(height ~ dnorm(mu, sigma),
        mu <- a + b * weight,
        a ~ dnorm(178, 100),
        b ~ dnorm(0, 10),
        sigma ~ dunif(0, 50)),
  data = dN
)
```

Now we plot 20 of these lines:

```{r}
# extract 20 samples from the posterior
post <- extract.samples(mN, n = 50)

# display raw data and sample size
plot(dN$weight, dN$height, xlim = range(d2$weight), ylim = range(d2$height),
     col = rangi2, xlab = "Weight", ylab = "Height")
mtext(concat("N = ", N))

# plot the lines, with transparency
for (i in 1:20) {
  abline(a = post$a[i], b = post$b[i], col = col.alpha("black", 0.3))
}
```

We could also plot an interval for uncertanty:

```{r}
mu_at_50 <- post$a + post$b * 50

dens(mu_at_50, col = rangi2, lwd = 2, xlab = "mu|weight = 50")

mu <- link(m4.3)
str(mu)
```

`link()` provides a posterior distribution for each indidivual in the data set, but we want a unique weight value for each $\mu$ on the horizontal axis:

```{r}
# define sequence of weights to compute predictions for these values will be 
# on the horizontal axis
weight.seq <- seq(from = 25, to = 70, by = 1)

# use link to compute mu for each sample from the posterior and for each weight
# in weight.seq
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)
```

There are 46 columns in `mu` because we gave it 46 different values for weight.

```{r}
# use type = "n" to hide raw data
plot(height ~ weight, d2, type = "n")

# loop over samples and plot each mu value
for (i in 1:100) {
  points(weight.seq, mu[i], pch = 16, col = col.alpha(rangi2, 0.1))
}

# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.89)

# plot raw data fading out points to make line and interval more visible
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))

# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)

# plot a shaded region for 89% HPDI
shade(mu.HPDI, weight.seq)
```

Here is the summary for generating predictions and intervals from the posterior of a fit model:

1. Use `link` to generate distributions of posterior values for $\mu$. The default behavior of `link` is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across. 

2. Use summary functions like `mean` of `HPDI` or `PI` to find averages and lower and upper bounds of $\mu$ for each value of the predictor variable.

3. Finally, use plotting functions like `lines` and `shade` to draws the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them.

Now let's plot predicted uncertainty:

```{r}
sim.height <- sim(m4.3, data = list(weight = weight.seq))
str(sim.height)

heights.pi <- apply(sim.height, 2, PI, prob = 0.89)
str(heights.pi)

# plot the raw data
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))

# draw MAP line
lines(weight.seq, mu.mean)

# draw HPDI region for line
shade(mu.HPDI, weight.seq)

# draw PI region for simulated heights
shade(heights.pi, weight.seq)
```

## Polynomial regression

```{r}
data("Howell1")
d <- Howell1
str(d)

plot(height ~ weight, data = d)
```

Now we standardize the variables:

```{r}
d$weight.s <- (d$weight - mean(d$weight)) / sd(d$weight)

plot(height ~ weight.s, data = d)
```

```{r}
d$weight.s2 <- d$weight.s^2

# Now refit the model
m4.5 <- map(
  alist(height ~ dnorm(mu, sigma),
        mu <- a + b1 * weight.s + b2 * weight.s2,
        a ~ dnorm(178, 100),
        b1 ~ dnorm(0, 10),
        b2 ~ dnorm(0, 10),
        sigma ~ dunif(0, 50)),
  data = d
)

# Results
precis(m4.5, corr = TRUE)
```

```{stan output.var = "m4.5.stan"}
data {
  int N;
  vector[N] height;
  vector[N] weight_s;
}

parameters {
  real a;
  real b1;
  real b2;
  real sigma;
}

model {
  // linear model
  vector[N] mu;
  mu = a + b1 * weight_s + b2 * (weight_s .* weight_s);
  
  // priors
  a ~ normal(178, 100);
  b1 ~ normal(0, 10);
  b2 ~ normal(0, 10);
  sigma ~ normal(0, 50);
  
  // likelihood
  height ~ normal(mu, sigma);
}
```

```{r}
dat <- list(N = NROW(d),
            height = d$height,
            weight_s = d$weight.s)

m4.5.stan.fit <- sampling(m4.5.stan,
                          data = dat)
precis(m4.5.stan.fit)

# posterior predictive checks
post <- as.data.frame(m4.5.stan.fit)
weight_s_new <- seq(-2.2, 2, length.out = 30)

f_mu <- function(x) post$a + post$b1 * x + post$b2 * (x ^ 2)

mu <- 
  sapply(weight_s_new, f_mu) %>%
  as_tibble() %>%
  rename_all(function(x) weight_s_new) %>%
  mutate(Iter = row_number()) %>%
  gather(weight_s, height, -Iter) %>%
  group_by(weight_s) %>%
  mutate(hpdi_l = HDInterval::hdi(height, credMass = 0.8)[1],
         hpdi_h = HDInterval::hdi(height, credMass = 0.8)[2]) %>%
  mutate(mu = mean(height)) %>%
  ungroup() %>%
  mutate(weight_s = as.numeric(weight_s))

sim_ht <- 
  sapply(weight_s_new,
         function(x)
           rnorm(NROW(post),
                 post$a + post$b1 * x + post$b2 * (x ^ 2),
                 post$sigma)) %>%
  as_tibble() %>%
  rename_all(function(x) weight_s_new) %>%
  mutate(Iter = row_number()) %>%
  gather(weight_s, height, -Iter) %>%
  group_by(weight_s) %>%
  mutate(pi_l = rethinking::PI(height, prob = 0.8)[1],
         pi_h = rethinking::PI(height, prob = 0.8)[2],
         mu = mean(height)) %>%
  ungroup() %>%
  mutate(weight_s = as.numeric(weight_s))

rescale <- seq(-2, 2, by = 1)

ggplot() +
  geom_point(data = d,
             aes(weight.s, height), shape = 1, color = 'dodgerblue') +
  geom_ribbon(data = sim_ht,
              aes(x = weight_s, ymin = pi_l, ymax = pi_h), alpha = .1) +
  geom_line(data = mu,
            aes(weight_s, mu)) +
  theme_pubr(base_family = 'sans') +
  scale_x_continuous(breaks = rescale,
                     labels = round(rescale * sd(d$weight) + mean(d$weight), 1)) +
  labs(x = 'Weight', y = "Height")
```


Let's plot the results:

```{r}
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight.s = weight.seq, weight.s2 = weight.seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
heights.PI <- apply(sim.height, 2, PI, prob = 0.89)

# plot
plot(height ~ weight.s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(heights.PI, weight.seq)
```

Now add in a cubic term:

```{r}
d$weight.s3 <- d$weight.s^3

# Now refit the model
m4.6 <- map(
  alist(height ~ dnorm(mu, sigma),
        mu <- a + b1 * weight.s + b2 * weight.s2 + b3 * weight.s3,
        a ~ dnorm(178, 100),
        b1 ~ dnorm(0, 10),
        b2 ~ dnorm(0, 10),
        b3 ~ dnorm(0, 10),
        sigma ~ dunif(0, 50)),
  data = d
)

# Results
precis(m4.6, corr = TRUE)
```

Let's plot the results:

```{r}
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight.s = weight.seq, weight.s2 = weight.seq^2, weight.s3 = weight.seq^3)
mu <- link(m4.6, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.6, data = pred_dat)
heights.PI <- apply(sim.height, 2, PI, prob = 0.89)

# plot
plot(height ~ weight.s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(heights.PI, weight.seq)
```
---
title: "Statistical Rethinking: Chapter 6 Information Theory"
output: html_notebook
---

**Overfitting** leads to poor prediction by learning too much from the data.

**Underfitting** leads to poor prediction by learning too little from the data.

To avoid these problems, there are two common approaches: 1) use a **regularizing prior** to tell the model not to get too excited by the data (same as penalized likelihood) and 2) use **information criteria** to model the prediction task and estimate predictive accuracy for some purpose. 

# The problem with parameters

## More parameters always improve fit

```{r}
sppnames <- c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", 
              "ergaster", "sapiens")

brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)

masskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)

d <- data.frame(species = sppnames, brain = brainvolcc, mass = masskg)
```

Fit the first model:

$$ v_i \sim \text{Normal}(u_i, \sigma) \\
u_i = \alpha + \beta_1m_i$$

```{r}
m6.1 <- lm(brain ~ mass, data = d)

# compute R^2
1 - var(resid(m6.1)) / var(d$brain)

summary(m6.1)
```

Now lets fit 5 polynomial models:

```{r}
m6.3 <- lm(brain ~ mass + I(mass^2) + I(mass^3), data = d)
m6.4 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4), data = d)
m6.5 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5), data = d)
m6.6 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6), data = d)
```

# Information theory and model performance

We want to measure accuracy of the model's predictions. This can be done using joint probability. For example, imagine the probabiliy of correctly predicting whether or not it will rain for the exact sequence of 5 days. This would involve calculating the probability correctly for each day and multiplying them all together. 

This joint probability is the likelihood in Bayes Theorem. It's the unique measure that correctly counts up the relative number of ways each event (sequence of rain and shine) could happen. Now, we need to measure distance from the established target. Information theory can help with this objective. 

**Information**: The reduction in uncertainty derived from learning an outcome. 

To use this definition, we need a way to quantify the uncertainty inherent in a probability distribution. Suppose there are two possible weather events on any particular day: either it is sunny or it is rainy. We want a function that uses the probabilities of shine and rain and produces a measure of uncertainty. The most common way to measure uncertianty is to name properties that a measure should possess:

1. the measure should be continuous.
2. The measure should increase as the number of possible events increases. 
3. The measure should be additive. 

There is only one function that meets these criteria. It is known as Information Entropy. If there are *n* different possible events and each event *i* has probability $p_i$, and we call the list of probabilities *p*, then the unique measure of uncertainty we seek is: 

$$
H(p) = -E \log(p_i) = - \sum^n_{i=1} p_i \log(p_i)
$$
That is, the uncertainty contained in a probability distribution is the average log-probability of an event. 

To compute the information entropy for the weather, suppose the true probabilities of rain and shine are $p_1 = 0.3$ and $p_2 = 0.7$, respectively. Then: 

$$
H(p) = -(p_1 \log(p_1) + p_2 \log(p_2)) \approx 0.61
$$

In R:

```{r}
p <- c(0.3, 0.7)
-sum(p * log(p))
```

Information entropy measures the uncertainty inherent in a distribution of events. 

### From entropy to accuracy

With H quantifying uncertainty,  we can now say, in a precise way, how hard it is to hit the target. To use information entropy to say how far a model is from the target, we ust divergence. 

**Divergence**: the additional uncertainty induced by using probabilities from one distribution to describe another distribution.

If the true distribution of events is $p_1 = 0.3$, $p_2 = 0.7$ but we guess that they happen with probabilities $q_1 = 0.25$, $q_2 = 0.75$, how much additional uncertianty have we introduced, as a consequence of using $q = \{q_1, q_2\}$ to approximate $p = \{p_1, p_2\}$? The answer:

$$ D_{kl}(p, q) = \sum_i p_i(\log(p_i) - \log(q_i)) = \sum_i p_i \log(\frac{p_i}{q_i}) $$

That is, the divergence is the average difference in log probability between the target $(p)$ and model $(q)$. This divergence is just the difference between the two entropies: the entropy of the target distribution $p$ and the cross entropy arising from using $q$ to predict $p$.

### From divergence to deviance

To approximate the relative value of $E\log(q_1)$ we can use a model's deviance, which is defined as:

$$D(q) = -2 \sum_i \log(q_1)$$

```{r}
# fit model with lm
m6.1 <- lm(brain ~ mass, d)

# compute deviance by cheating
(-2) * logLik(m6.1)
```

### From deviance to out-of-sample

We are interested in the deviance on new data. When we usually have data and use it to fit a statistical model, the data comprise a training sample. Parameters are estimated from it, and then we can imagine using thse estimates to predict outcomes in a new sample, called the test sample. Here is the precedure:

1. Suppose there's a training sample of size N
2. fit a model to the training sample, and compute the deviance on the training sample. Call this deviance $D_{train}$
3. Suppose another sample of size N from the same process. This is the test sample.
4. Compute the ddeviance on the test sample. This means using the MAP estimates from step (2) to compute the deviance for the data in the test sample. Call this deviance $D_{test}$. 


# Regularization

Overfitting means that the machine learns too much from the data. One way to slow this process is to give it a skeptical or regularizing prior. 


# Information Criteria

AIC provides an approximation of predictive accuracy, as measured by out-of-sample  deviance. All information criteria aim at the same target, but are derived under more and less general assumptions. AIC is an approximation that is reliable only when: 

1. the priors are flat or overwhelmed by the likelihood.
2. the posterior distribution is approximately multivariate Gaussian
3. the sample size N is much greater than the number of parameters k

Since flat priors are hardly ever the best, we will focus on Devience Information Criterion (DIC) and Widely Applicable Information Criterion (WAIC). DIC is essentially a version of AIC that recgonizes informed priors. 

### WAIC

WAIC is calculated as taking the averages of log-likelihood over the posterior distribution. 


# Using information criteria

### Model comparison

* Model comparison means using DIC/WAIC in combination with the estimates and posterior predictive checks from each model. 

```{r}
library(rethinking)
data(milk)
d <- milk[complete.cases(milk), ]
d$neocortex <- d$neocortex.perc / 100
dim(d)
```

```{r}
a.start <- mean(d$kcal.per.g)

# restrict sigma to be positve
sigma.start <- log(sd(d$kcal.per.g))

m6.11 <- map(
  alist(kcal.per.g ~ dnorm(a, exp(log.sigma))),
  data = d, 
  start = list(a = a.start, log.sigma = sigma.start)
)

m6.12 <- map(
  alist(kcal.per.g ~ dnorm(mu, exp(log.sigma)),
        mu <- a + bn * neocortex),
  data = d, 
  start = list(a = a.start, bn = 0, log.sigma = sigma.start)
)

m6.13 <- map(
  alist(kcal.per.g ~ dnorm(mu, exp(log.sigma)),
        mu <- a + bm * log(mass)),
  data = d, 
  start = list(a = a.start, bm = 0, log.sigma = sigma.start)
)

m6.14 <- map(
  alist(kcal.per.g ~ dnorm(mu, exp(log.sigma)),
        mu <- a + bn * neocortex + bm * log(mass)),
  data = d, 
  start = list(a = a.start, bn = 0, bm = 0, log.sigma = sigma.start)
)
```

#### Compare WAIC values

```{r}
WAIC(m6.14)

milk.models <- compare(m6.11, m6.12, m6.13, m6.14)
milk.models
plot(milk.models, SE = TRUE, dSE = TRUE)
```

#### Comparing estimates

```{r}
coeftab(m6.11, m6.12, m6.13, m6.14)

plot(coeftab(m6.11, m6.12, m6.13, m6.14))
```



### Model averaging

* Model average means using DIC/WAIC to construct a posterior predictive distribution that exploits what we know about relative accuracy of the models. 

Let's simulate and plot counterfactual prediction for the minimum-WAIC model.

```{r}
# compute counterfactual predictions
# neocortex from 0.5 to 0.8
nc.seq <- seq(from = 0.5, to = 0.8, length.out = 30)
d.predict <- list(
  kcal.per.g = rep(0, 30), 
  neocortex = nc.seq,
  mass = rep(4.5, 30)
)

pred.m6.14 <- link(m6.14, data = d.predict)
mu <- apply(pred.m6.14, 2, mean)
mu.PI <- apply(pred.m6.14, 2, PI)

# plot it all
plot(kcal.per.g ~ neocortex, d, col = rangi2)
lines(nc.seq, mu, lty = 2)
lines(nc.seq, mu.PI[1, ], lty = 2)
lines(nc.seq, mu.PI[2, ], lty = 2)
```

Now let's compute and add model averaged posterior predictions. 

1. Compute WAIC for each model.
2. Compute the weight for each model.
3. Compute linear model and simulated outcomes for each model.
4. Combine these values into an ensemble of predictions, using the model weights as proportions. 

```{r}
milke.ensemble <- ensemble(m6.11, m6.12, m6.13, m6.14, data = d.predict)
mu <- apply(milke.ensemble, 2, mean)
mu.PI <- apply(milke.ensemble, 2, PI)
lines(nc.seq, mu)
shade(mu.PI, nc.seq)
```










